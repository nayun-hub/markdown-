# 线性回归

## 基本要素

### 模型定义

线性回归假设输出和各输入之间的线性关系
$$
\widehat{y} = x_1w_1+x_2w_2+b
$$
其中，w1和w2是权重，b是偏差。

### 模型训练

* 训练数据

  训练集，样本，标签，特征

* 损失函数

  衡量预测值与真实值之间的误差，通常为平方函数（平方损失），评估索引i的样本误差表达式
  $$
  l^{(i)}(w_1,w_2,b)=\frac{1}{2}(\widehat{y}^{(i)}-y^{(i)})^2
  $$
  1/2的作用是使其求导后常系数为1。

  通常，我们用训练数据集中所有样本误差的平均来衡量模型预测质量
  $$
  l(w_1,w_2,b)=\frac{1}{n}\sum^n_{i=1}l^{(i)}(w_1,w_2,b)=\frac{1}{n}\sum^n_{i=1}\frac{1}{2}(x_1^{i}w_1+x_2^iw_2+b-y^{(i)})^2
  $$
  在模型训练中，希望找出一组模型参数,使训练样本平均损失误差最小：
  $$
  w^*_1,w^*_2,b=arg min l(w_1,w_2,b)
  $$
  
* 优化算法

**小批量随机梯度下降**

> 先随机选取一组模型参数的初始值，接下来对参数进行多次迭代，使每次迭代都有可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数⽬目训练数据样本所组成的⼩批量（mini-batch） ，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后用此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。  

在上述模型中
$$
w_1\leftarrow {w_1}-\frac{\eta}{|B|}\sum_{i\in{B}}\frac{\partial{l^{i}(w_1,w_2,b)}}{\partial{w_1}}\\
w_2\leftarrow {w_2}-\frac{\eta}{|B|}\sum_{i\in{B}}\frac{\partial{l^{i}(w_1,w_2,b)}}{\partial{w_2}}\\
w_3\leftarrow {w_3}-\frac{\eta}{|B|}\sum_{i\in{B}}\frac{\partial{l^{i}(w_1,w_2,b)}}{\partial{w_3}}
$$
B代表每个小批量中的样本数据，η为学习率。