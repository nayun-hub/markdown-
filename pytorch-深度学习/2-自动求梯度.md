# 自动求梯度

pytorch提供的**autograd**包能够根据输入和前向传播过程自动构建计算图，并执行反向传播。

> 前向传播：前向传播过程就是数据从输入层传入，经过隐藏层，最终到达输出层的过程。

> 反向传播：核心在于复合函数的链式求导法则，反向传播的作用在于优化代价函数，这也是训练神经网络的目标。

## 概念

**Tensor**是**autograd包**的核心类，如果将其属性**.requires_grad**设置为True，将开始追踪其上的所有操作。完成计算后，可以调用**.backward()**来完成梯度计算，梯度将积累到**.grad**属性中。

如果不想继续追踪，可以调用**.detach()**将其从追踪记录中分离出来，防止计算被追踪。此外，还可以调用**with torch.no_grad()**将不想被追踪的操作代码包裹起来，在评估模型是很常用。

**Function**是另一个很重要的类。**Tensor和Function互相结合**可以构建一个记录有整个计算过程的**有向无环图（DAG）**。每个Tensor都有一个**.grad_fn**属性，**该属性即创建该Tensor的Function**。（即，该Tensor是不是通过某些运算得到的，若是，则grad_fn返回一个与折现运算相关的对象，否则是None）

## TENSOR

```python
x = torch.ones(2, 2, requires_grad=True)
print(x)
print(x.grad_fn)
# 输出
tensor([[1., 1.],
        [1., 1.]], requires_grad=True)
None
```

再做一下运算

```python
y = x + 2
print(y)
print(y.grad_fn)
# 输出
tensor([[3., 3.],
        [3., 3.]], grad_fn=<AddBackward0>)
<AddBackward0 object at 0x000002A7D97523C8>
```

x是直接创建的，所以它的.grad_fn属性为None, y是通过计算得到的，所以它的.grad_fn属性是<AddBackward0>

将x称为叶子节点(**.is_leaf**)

再做复杂点的运算

```python
z = y * y * 3
out = z.mean()
print(z, out)
# 输出
tensor([[27., 27.],
        [27., 27.]], grad_fn=<MulBackward0>) 
tensor(27., grad_fn=<MeanBackward0>)
```

## 梯度

```python
x = torch.ones(2, 2, requires_grad=True)
y = x + 2
z = y * y * 3
out = z.mean()
out.backward()				# out是一个标量，不需要指定求导变量
print(x.grad)
# 输出
tensor([[4.5000, 4.5000],
        [4.5000, 4.5000]])
```

$$
out=\frac{1}{4}\sum^4_{i=1}z_i=\frac{1}{4}\sum^4_{i=1}3(x_i+2)^2 \tag{1}
$$

所以
$$
\frac{\partial{out}}{\partial{x_i}}=\frac{9}{2}=4.5 \tag{2}
$$
进一步

```python
import torch

x = torch.ones(2, 2, requires_grad=True)
y = x + 2
z = y * y * 3
out = z.mean()
out.backward()
print(x.grad)
# 反向传播一次
out2 = x.sum()
out2.backward()
print(x.grad)

out3 = x.sum()
# 没调用一遍 .backward()函数就会积累一遍梯度
# 将之前梯度累积值请零
x.grad.data.zero_()
out3.backward()
print(x.grad)

# 输出
tensor([[4.5000, 4.5000],
        [4.5000, 4.5000]])
tensor([[5.5000, 5.5000],
        [5.5000, 5.5000]])
tensor([[1., 1.],
        [1., 1.]])
```

**特别的，对于z.backward(), 若z为标量，则不需要传入任何参数。若z为张量，则需要传入一个与z同形的张量v，作l = torch.sum(z*v),得到标量l，然后再用l对自变量求梯度**

示例：

```python
x = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)
y = x * 2
z = y.view(2, 2)
print(z)
v = torch.tensor([[1.0, 0.1],[0.01, 0.001]], dtype=torch.float)
z.backward(v)
print(torch.sum(z*v))
print(x.grad)
# 输出
tensor([[2., 4.],
        [6., 8.]], grad_fn=<ViewBackward>)
tensor(2.4680, grad_fn=<SumBackward0>)
tensor([2.0000, 0.2000, 0.0200, 0.0020])
```

 **中断梯度追踪**

```python
x = torch.tensor(1.0, requires_grad=True)
y1 = x ** 2
with torch.no_grad():
    y2 = x ** 3
y3 = y1 + y2
y3.backward()
print(x.grad)
# 输出
tensor(2.)
```

在执行 “torch.no_grad()”时，所有计算得出的tensor的requires_grad都自动设置为False。

即此时y2的"requires_grad"=False, 求y3对x的梯度时y2不参与计算。

```python
# 关于tensor.data
x = torch.ones(1, requires_grad=True)
# tensor.data 会返回tensor的值，但不会加入到ensor的计算中
print(x.data)
print(x.data.requires_grad)

y = 2 * x
# 改变x.data的值，但不影响x
x.data *= 100

y.backward()
print(x)
print(x.grad)
# 输出
tensor([1.])
False
tensor([100.], requires_grad=True)
tensor([2.])
```

