# 线性回归的从零开始实现

## 生成数据集

设训练数据集样本数为1000，输入个数（特征数）为2。使用线性回归模型真实权重2、-3.4 ，偏差b=4.2 ，以及一个随机噪声项来生成偏差。
$$
y=Xw+b+\epsilon
$$
噪声服从均值为0，标准差为0.01的正态分布。

```python
import torch
from IPython import display
from matplotlib import pyplot as plt
import numpy as np
import random

num_inputs = 2
num_examples = 1000
true_w = [2, -3.4]
true_b = 4.2
# 高斯分布均值0，方差1，返回size(num_examples, num_inputs)
features = torch.from_numpy(np.random.normal(0, 1, (num_examples, num_inputs))).to(torch.float32)
# print(features)
labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b
labels += torch.from_numpy(np.random.normal(0, 1, size=labels.size()))
print(features[0], labels[0])

def use_svg_display():
    # 用矢量图显示
    display.set_matplotlib_formats('svg')

def set_figsize(figsize=(3.5, 2.5)):
    use_svg_display()
    # 设置图的尺寸大小
    # rc参数可以修改默认的属性
    plt.rcParams['figure.figsize'] = figsize
    
set_figsize()
plt.scatter(features[:, 1].numpy(), labels.numpy(), 1)
plt.show()

# 输出
tensor([-1.3521,  1.3017], dtype=torch.float64) tensor(-2.5194, dtype=torch.float64)
```

![生成数据集](C:\Users\nayun\Desktop\project_file\markdown-笔记\pytorch-深度学习\线性回归-图1.png)

## 读取数据

在训练模型时需要不断遍历数据集并读取小批量数据。这里定义一个函数每次返回batch_size个随机样本的特征和标签。

```python
def data_iter(batch_size, features, labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    random.shuffle(indices)
    for i in range(0, num_examples, batch_size):
        # 最后一次可能不足一个batch_size
        j = torch.LongTensor(indices[i: min(i+batch_size, num_examples)])
        yield features.index_select(0, j), labels.index_select(0, j)
```

补充知识：

```python
a = torch.Tensor([[[1, 2, 3],
                   [4, 5, 6],
                   [7, 8, 9]]])
# tensor.index_select(self, dim, index:LongTensor) dim:需要对输入张量进行索引的维度 index:索引
b = a.index_select(0, torch.LongTensor([0]))
c = a.index_select(1, torch.LongTensor([0]))
d = a.index_select(2, torch.LongTensor([0]))
print(b)
print(c)
print(d)

#输出
tensor([[[1., 2., 3.],
         [4., 5., 6.],
         [7., 8., 9.]]])
tensor([[[1., 2., 3.]]])
tensor([[[1.],
         [4.],
         [7.]]])
```

## 初始化模型参数

假设，将权重初始化为均值为0，标准差为0.01的正态随机数，偏差初始化为0 。

```python
w = torch.tensor(np.random.normal(0, 0.01, size=(num_inputs, 1)), dtype=torch.float32)
b = torch.zeros(1, dtype=torch.float32)
# 需要对参数求梯度来迭代参数的值
w.requires_grad_(requires_grad=True)
b.requires_grad_(requires_grad=True)
```

## 定义模型

使用mm函数做矩阵乘法

```python
def linereg(x, w, b):
    reture torch.mm(x, w) + b
```

## 定义损失函数

使用平方损失作为损失函数

```python
def squard_loss(y_hat, y):
    # 真实值为y，预测值为y_hat
    # 返回的是向量
    return (y_hat-y.view(y_hat.size()))**2/2
```

## 小批量随机梯度下降法

```python
def sgd(params, lr, batch_size):
    for param in params:
        param.data -= lr * param.grad / batch_size
```

## 训练模型

在训练中，多次迭代模型参数。我们根据当前读取的小批量数据样本（特征x和标签y），通过调用反向函数**backward**计算批量随机梯度，并调用优化算法**sgd**迭代模型参数。由于之前设置批量大小**batch_size**为10，每个小批量的损失**l**的形状为（10，l）。由于变量**l**并不是标量，所以可以调用**.sum()**将其求和得到一个标量，再运行**l.backward()**得到该变量有关模型参数的梯度。每次更新完参数要记得将参数梯度清零。

在一个迭代周期(epoch)中，我们将完整遍历一遍**data_iter**函数，并对训练数据集中所有样本都使用一次。迭代周期个数**num_epochs**和学习率**lr**都是超参数，分别设为3和0.03 。

```python
lr = 0.03
num_epochs = 3
net = linereg
loss = squard_loss
# 训练模型一共需要num_epochs个迭代周期
for epoch in range(num_epochs):
    # 每个迭代周期中，会使用训练数据集中所有样本一次
    for x, y in data_iter(batch_size, features, labels):
        line_net = net(x, w, b)
        l = loss(line_net, y).sum()
        l.backward()
        sgd([w, b], lr, batch_size)
        # 梯度清零
        w.grad.data.zero_()
        b.grad.data.zero_()
    train_l = loss(net(features, w, b), labels)
    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().item()))
# 训练完之后，输出学习得到的参数
print(true_w, '\n', w)
print(true_b, '\n', b)

# 输出
epoch 1, loss 0.578562
epoch 2, loss 0.537274
epoch 3, loss 0.537320
[2, -3.4] 
 tensor([[ 1.9665],
        [-3.3884]], requires_grad=True)
4.2 
 tensor([4.1980], requires_grad=True)
```

